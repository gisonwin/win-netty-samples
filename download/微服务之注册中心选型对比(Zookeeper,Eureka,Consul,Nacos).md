# 微服务之注册中心选型对比(Zookeeper,Eureka,Consul,Nacos)

### 一.前言

​	注册中心本质上是为了解耦服务提供者和服务消费者.对于任意微服务,原则上都应该存在或支持多个提供者,这是微服务的分布式属性决定的.为了支持弹性扩容特性,一个微服务的提供者的数量和分布往往是动态变化,也是无法预先确定的.所以原来在单体应用中常用的LB机制就不再适用,需要额外引入组件来管理微服务的提供者的注册与发现,我们称这个组件为服务注册中心.

### 二.CAP理论

​	CAP理论是分布式架构中重要的理论

> - 一致性(Consistency):所有节点在同一时间具有相同的数据
> - 可用性(Availability):保证每个请求不管成功或失败都有响应.
> - 分区容忍性(Partition tolerance):系统中任意信息的丢失或失败不会影响系统的继续运行.

​	我们来说一下P,分区容忍性.在整个系统中的某个部分无响应或宕机了,但并不影响整个系统的运行和使用.

​	CAP理论,在现实世界中我们无法同时满足三个条件,只能选其中两种,由P是必须的,所以产生了CP和AP两种.如果同时满足AC,则分区容错很难保证了,就会产生单点.

### 三.解决方案

​	架构师或CTO在服务注册中心选型时,首先要考虑的就是服务发现与注册机制.我们将当前各主流的服务注册中心解决方案大致分为如下三类:

- 应用内:直接集成到应用中,依赖于应用自身完成服务的注册与发现,典型应用就是Netflix提供的Eureka.
- 应用外:把应用当成黑盒,通过应用外的某种机制将服务注册到注册中心,最小化对应用的侵入性,典型应用如HashiCorpr的Consul.
- DNS:将服务注册为DNS的SRV记录,可以将它看作一种特殊的应用外注册方式,SkyDNS为典型代表.

 除了基本的服务注册与发现机制,还要考虑至少以下五个方面问题:

- 测活:服务注册之后,如何对服务进行测活以保证服务的可用性?
- 负载均衡:当存在多个服务提供者时,如何均衡各个提供者的负载?
- 集成:在服务提供端或消费商,如何集成注册中心?
- 运行时依赖:引用注册中心之后,对应用的运行时环境有何影响?
- 可用性:如何保证注册中心本身的可用性,如何消除单点故障陷患?

### 四.主流框架产品

|                  | Nacos                      | Eureka      | Consul            | Zookeeper  | CoreDNS    |
| ---------------- | -------------------------- | ----------- | ----------------- | ---------- | ---------- |
| 一致性协议       | CP+AP                      | AP          | CP                | CP         | --         |
| 健康检查         | TCP/HTTP/MySQL/Client Beat | Client Beat | TCP/HTTP/gRPC/Cmd | Keep Alive | --         |
| LB策略           | weight/metadata/Selector   | Ribbon      | Fabio             | ---        | RoundRobin |
| 雪崩保护         | 有                         | 有          | 无                | 无         | 无         |
| 自动注销实例     | 支持                       | 支持        | 不支持            | 支持       | 不支持     |
| 访问协议         | HTTP/DNS                   | HTTP        | HTTP/DNS          | TCP        | DNS        |
| 监听支持         | 支持                       | 支持        | 支持              | 支持       | 不支持     |
| 多数据中心       | 支持                       | 支持        | 支持              | 不支持     | 不支持     |
| 跨注册中心同步   | 支持                       | 不支持      | 支持              | 不支持     | 不支持     |
| Spring Cloud集成 | 支持                       | 支持        | 支持              | 不支持     | 不支持     |
| Dubbo集成        | 支持                       | 不支持      | 不支持            | 支持       | 不支持     |
| K8S集成          | 支持                       | 不支持      | 支持              | 不支持     | 支持       |

#### Apache Zookeeper(CP):

​	Apache Zookeeper在设计时就遵循CP原则,即任何时间对Zookeeper的访问请求能得到一致的数据结果,同时系统对网络分割具备容错性,但是Zookeeper不能保证每次服务请求都是可达的.比如在使用Zookeeper获取服务列表时,如果此时Zookeeper集群中的Leader宕机,该集群就要进行Leader选举,或Zookeeper集群中半数以上服务器节点不可用(假设有三个节点,如果节点一检测到节点三挂了,节点二也检测到节点三挂了,则节点三才算是真挂了),那么将无法处理该请求,所以Zookeeper不能保证服务可用性.

​	在分布式环境中数据存储场景下,数据一致性应该是首先被保证的,所以这也是ZK遵循CP的原因之一.但是在服务消费者来说,能消费才是最重要的,消费者虽然拿到可能不正确的服务实例信息后尝试消费一下,也要胜过因为无法获取实例信息而不能去消费,导致系统异常要好(淘宝的双十一,京东的618就是AP最好的写照).当ZK的master节点因为网络故障与其他节点失去联系时,剩余节点会重新进行leader选举.问题在于选举leader的时间太长,30s~120s,由于选举期间整个zk集群都是不可用的,会导致选举期间注册服务失灵.而在云部署环境下,因为网络问题使得zk集群失去master节点是大概率事件,虽然服务最终能恢复,但是漫长的选举事件导致注册长期不可用是不可容忍的.

#### Spring Cloud Eureka(AP):

​		Netflix在设计Eureka时遵循AP原则(2.0版本闭源),Eureka可以运行多个实例来构建集群解决单点故障问题,Eureka Server采用的是Peer to Peer对等通信,这是一种云中心化的架构,无master/slave之分,每个peer都是对等的.这种架构风格中,节点通过彼此互相注册来提高可用性,每个节点需要添加一个或多个有效的serviceurl指向其他节点.每个节点都被视为其他节点的副本.

​		集群环境下某如Eureka Server宕机,Eureka Client请求会自动切换到新的Server节点上,当宕机机器恢复后,Eureka会再次将其纳入服务器集群管理中.当新节点开始接受客户端请求时,所有操作都会在节点间进行复制(replicate to peer)操作,将请求复制到该Eureka Server当前所知的其他所有节点中.当一个新的Eureka Server节点启动后,会首先尝试从邻近节点获取所有注册列表信息,并完成初始化.Server通过getEurekaServiceUrls()方法获取所有节点,且通过心跳契约的方式定期更新.默认情况下Eureka Server在一定时间内没接到某个服务实例的心跳(默认周期30秒),Eureka将会注销该实例(默认为90秒,eureka.instance,lease-expiration-duration-in-seconds设置).当Eureka Server节点在短时间内丢失过多心跳时,该节点会进入自我保护模式.

​		Eureka集群中,只要有一台Eureka还在,就能保证注册服务可用(保证可用性),只不过查到的信息有可能不是最新的(不保证强一致性).此外,Eureka还有一种自我保护机制,如果在15分钟内超过85%的节点都没有正常心跳,Eureka就认为客户端与注册中心出现了网络故障,此时会出现以下几种情况:

1. Eureka不再从注册表中移除因为长时间没有收到心跳而过期的服务;
2. Eureka仍然能够接受新服务注册和查询请求,但是不会被同步到其他节点上(只保证当前节点依然可用);
3. 当网络稳定时,当前实例新注册的信息会被同步到其他节点中.

==所以Eureka可以很好的应对因网络故障导致部分节点失去联系的情况,而不会像zk使得整个注册服务瘫痪.==

#### HashiCorp Consul(CP):

​		Consul使用Go语言编写,具有天然可移植性(支持Linux,Windows,Max OS X),用于实现分布式系统的服务发现与配置.它内置了服务注册与发现框架,分布式一致性协议实现健康检查,KV存储,多数据中心方案,不再需要依赖其他工具,使用也较方便简单.它遵循了CAP原理中CP原则,保证了强一致性和分区容错性,使用的Raft算法,比zk的Paxos算法更加简单.但是Consul的可用性不高,比如服务注册的时间会稍长一些,这是和Raft协议有关,Raft协议要求必须过半数的节点都写入成功才认为注册成功,在leader挂掉之后,重新选举出leader之前会导致Consul服务不可用.

​	Consule强一致性带来的是:

1. 服务注册相比Eureka会稍慢一些,是由Raft协议决定的,Raft协议要求必须过半数节点都写入成功才认为注册成功
2. Leader挂掉时,重新选举期间,整个Consule不可用.虽然保证了强一致性,但牺牲了可用性.

Eureka保证高可用性和最终一致性:

1. 服务注册相对要快,因为不需要等注册信息replicate到其他节点,也不保证注册信息是否replicate成功
2. 当数据出现不一致时,虽然A,B上注册信息不完全相同,但每个Eureka节点依然能够正常对外提供服务,这会出现查询服务信息时,如果请求A查不到,但请求B可以查到.所以保证了可用性,但牺牲了一致性.

Eureka是个Servlet程序,跟在Servlet容器中,而Consul则Go编写而成.

#### Alibaba Nacos(AP+CP):

​		Nacos是阿里开源的,支持基于DNS和基于RPC的服务发现.在Spring Cloud中使用Nacos,只需要先下载Nacos并启动Nacos Server,Nacos只需要简单配置就可以完成服务的注册发现.另外还支持动态配置服务.动态配置服务可以让您以中心化,外部化和动态化的方式管理所有环境的应用配置和服务配置.动态配置消除了配置变更时重新部署应用和服务的需要,让配置管理变得更加高效和敏捷,配置中心化管理让实现无状态服务变得更简单,让服务按需弹性扩容更容易.

​		Nacos=Spring Cloud 注册中心+Spring Cloud 配置中心



























